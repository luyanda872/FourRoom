# ğŸ¤– Reinforcement Learning in FourRooms Domain

<div align="center">
  
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)
![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)

**ğŸ¯ Advanced Q-Learning Implementation | ğŸŒŸ Multi-Scenario Training | ğŸ”¬ Stochastic & Deterministic Modes**

</div>

---

## ğŸš€ Project Overview

This project implements **sophisticated Q-learning agents** in the classic FourRooms grid environment, exploring different package collection scenarios with increasing complexity. Features both **deterministic** and **stochastic** environments with advanced epsilon-greedy exploration strategies.

> **âš¡ Windows Compatible** - Fully tested on Windows with GIT Bash support!

---

## ğŸ® Scenario Breakdown

<div align="center">

| ğŸ¯ Scenario | ğŸ“¦ Task | ğŸ§  Strategy | ğŸ² Difficulty |
|-------------|---------|-------------|----------------|
| **Scenario 1** | Collect **1 package** | Fixed & Decaying Îµ | â­ Beginner |
| **Scenario 2** | Collect **3 packages** (any order) | Decaying Îµ | â­â­ Intermediate |
| **Scenario 3** | Collect **3 color-coded packages** (Râ†’Gâ†’B) | Decaying Îµ | â­â­â­ Advanced |

</div>

---

## ğŸ“ Project Architecture

```
FourRooms-RL/
â”œâ”€â”€ ğŸ  FourRooms.py         # Core environment (predefined)
â”œâ”€â”€ ğŸ¯ Scenario1.py         # Single package collection
â”œâ”€â”€ ğŸ¯ Scenario2.py         # Multi-package collection
â”œâ”€â”€ ğŸ¯ Scenario3.py         # Sequential color-coded collection
â”œâ”€â”€ ğŸ“„ requirements.txt     # Python dependencies
â”œâ”€â”€ ğŸ“„ Makefile            # Automated commands
â””â”€â”€ ğŸ“„ README.md           # Project documentation
```

### ğŸ”§ File Descriptions

#### `FourRooms.py`
- **Core Environment** - Predefined grid world (do not modify)
- **State Management** - Handles agent positioning and package tracking
- **Action Processing** - Manages movement and collection mechanics

#### `Scenario1.py` 
- **Single Package Collection** - Learn to collect 1 package efficiently
- **Dual Strategy Support** - Both fixed and decaying epsilon
- **Baseline Performance** - Foundation for complex scenarios

#### `Scenario2.py`
- **Multi-Package Collection** - Collect 3 packages in any order
- **Strategic Planning** - Agent learns optimal collection routes
- **Decaying Exploration** - Sophisticated epsilon scheduling

#### `Scenario3.py`
- **Sequential Collection** - Collect Râ†’Gâ†’B packages in strict order
- **Advanced Planning** - Complex state-action mapping
- **Constraint Satisfaction** - Order-dependent reward structure

---

## ğŸ§  Exploration Strategies

### ğŸ¯ Fixed Epsilon (Îµ = 0.1)
- **Constant Exploration**: Maintains steady 10% random action rate
- **Stable Learning**: Consistent exploration throughout training
- **Best For**: Simple environments with clear optimal paths

### ğŸ“‰ Decaying Epsilon (Îµ = 1.0 â†’ 0.05)
- **Adaptive Exploration**: Starts with 100% exploration, decays to 5%
- **Efficient Learning**: High initial exploration, then exploitation
- **Best For**: Complex environments requiring strategic planning

---

## ğŸ® Quick Start Guide

### ğŸ”§ Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or use Makefile
make setup
```

### ğŸš€ Running Scenarios

#### Deterministic Mode (Predictable Actions)
```bash
python Scenario1.py
python Scenario2.py
python Scenario3.py
```

#### Stochastic Mode (20% Action Randomness)
```bash
python Scenario1.py --stochastic
python Scenario2.py --stochastic
python Scenario3.py --stochastic
```

---

## ğŸ› ï¸ Makefile Commands

> **ğŸ’» Windows Users**: Use GIT Bash for full compatibility!

### Setup & Dependencies
```bash
make setup          # Install all requirements
```

### Scenario Execution
```bash
make run1           # Scenario 1 (Deterministic)
make run1-stoch     # Scenario 1 (Stochastic)
make run2           # Scenario 2 (Deterministic)
make run2-stoch     # Scenario 2 (Stochastic)
make run3           # Scenario 3 (Deterministic)
make run3-stoch     # Scenario 3 (Stochastic)
```

### Environment Management
```bash
make clean          # Clean generated files
```

---

## ğŸ“Š Generated Outputs

Each scenario produces comprehensive analysis files:

### ğŸ“ˆ Learning Curves
- `learning_curves_both_<mode>.png` - Training progress visualization
- **Tracks**: Episode rewards, convergence rate, exploration decay

### ğŸ—ºï¸ Path Visualization
- `final_path_<strategy>_<mode>.png` - Agent's optimal path
- **Shows**: Start/end positions, package locations, learned route

### ğŸ“ Execution Logs
```plaintext
ğŸ¯ Training Progress:
Episode 1000: Average Reward = 15.2, Epsilon = 0.1
Episode 2000: Average Reward = 18.7, Epsilon = 0.05

ğŸš€ Final Run:
[Step 4] Action: RIGHT -> (5, 6) | Grid: RED | Packages left: 2
ğŸ“¦ Package collected!
ğŸ Reached terminal state
```

---

## ğŸ”¬ Technical Features

### ğŸ¯ Q-Learning Implementation
- **Temporal Difference Learning**: Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
- **Dynamic Learning Rate**: Adaptive Î± based on state visitation
- **Discount Factor**: Î³ = 0.95 for future reward consideration

### ğŸŒŸ Advanced Exploration
- **Epsilon Scheduling**: Linear decay with minimum threshold
- **Action Selection**: Balanced exploration vs exploitation
- **State Space Coverage**: Efficient environment mapping

### ğŸ“Š Performance Metrics
- **Convergence Analysis**: Training stability assessment
- **Path Optimization**: Route efficiency measurement
- **Success Rate**: Task completion percentage

---

## ğŸ¯ Environment Modes

### ğŸ”’ Deterministic Mode
- **Predictable Actions**: Agent moves exactly as intended
- **Consistent Results**: Reproducible learning outcomes
- **Ideal For**: Algorithm testing and baseline performance

### ğŸ² Stochastic Mode
- **20% Action Randomness**: Adds realistic uncertainty
- **Robust Learning**: Handles environmental noise
- **Real-World Simulation**: More challenging and realistic

---

## ğŸ“‹ Requirements

- **Python 3.7+**
- **NumPy**: Numerical computations
- **Matplotlib**: Visualization and plotting
- **Windows**: GIT Bash recommended for Makefile support

---

## ğŸŒŸ Key Achievements

- âœ… **Multi-Scenario Learning**: Progressive difficulty scaling
- âœ… **Dual Environment Support**: Deterministic & stochastic modes
- âœ… **Advanced Exploration**: Fixed & decaying epsilon strategies
- âœ… **Comprehensive Visualization**: Learning curves & path analysis
- âœ… **Cross-Platform Support**: Windows-compatible automation
- âœ… **Professional Documentation**: Clear setup and usage guide

---

## ğŸš€ Future Enhancements

- ğŸ”„ **Deep Q-Learning**: Neural network-based Q-function
- ğŸ¯ **Multi-Agent Systems**: Collaborative package collection
- ğŸŒ **Custom Environments**: User-defined grid configurations
- ğŸ“Š **Advanced Metrics**: Detailed performance analytics

---

## ğŸ¤ Contributing

Feel free to fork this project and submit pull requests for improvements!

---

## ğŸ“§ Contact

**Luyanda**  
ğŸ“§ MBLLUY007@myuct.ac.za

---

<div align="center">
  
**â­ Star this repo if you found it helpful!**

*Building intelligent agents, one Q-value at a time* ğŸ¤–

</div>